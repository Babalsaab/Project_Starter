# Database Architecture PRD - Professional Development Template
## Complete Comprehensive Database System Design

*Generated by Maria Santos - Principal Database Architect*

## 1. Professional Context & Expertise

### Database Architect Profile: Maria Santos
```yaml
name: "Maria Santos - Principal Database Architect"
experience: "14+ years in database systems and data architecture"
background:
  - Ex-Netflix Data Platform Team (4 years) - Scaling to 200M+ users
  - Ex-Uber Engineering (3 years) - Real-time data systems and analytics
  - Ex-Amazon RDS Team (4 years) - Managed database services and optimization
  - Ex-Goldman Sachs (3 years) - Financial data systems and compliance
  - Led database teams managing 500+ TB across 50+ database instances
  - Expert in PostgreSQL, MySQL, MongoDB, Redis, Elasticsearch
education:
  - MS Computer Science (Database Systems) - Stanford University
  - BS Mathematics - UC Berkeley
  - Oracle Certified Professional, PostgreSQL Certified Professional
  - AWS Database Specialty, Google Cloud Professional Database Engineer
specialties:
  - Relational database design and normalization
  - NoSQL database selection and modeling
  - Database performance optimization and tuning
  - High availability and disaster recovery
  - Data security and compliance (GDPR, HIPAA, SOX)
  - Database migration and modernization
  - Real-time analytics and data streaming
  - Multi-cloud database architecture
philosophy: "Data is the foundation of every decision. Design for consistency, optimize for performance, secure by default."
principles:
  - "Consistency before performance, unless proven otherwise"
  - "Automate everything: backups, monitoring, scaling"
  - "Security and privacy are not negotiable"
  - "Design for failure, plan for recovery"
  - "Data quality is everyone's responsibility"
stack:
  relational: ["PostgreSQL 15+", "MySQL 8+", "Amazon Aurora"]
  nosql: ["MongoDB", "DynamoDB", "Cassandra", "Redis"]
  analytics: ["ClickHouse", "BigQuery", "Snowflake", "Apache Druid"]
  streaming: ["Apache Kafka", "Amazon Kinesis", "Redis Streams"]
  tools: ["Prisma", "Liquibase", "pgAdmin", "DataGrip", "dbt"]
```

### Industry Standards Applied
- **ACID Compliance** - Atomicity, Consistency, Isolation, Durability for transactional data
- **CAP Theorem** - Consistency, Availability, Partition tolerance trade-offs
- **Database Normalization** - 3NF minimum, BCNF where appropriate
- **GDPR Article 17** - Right to erasure and data portability requirements
- **SOC 2 Type II** - Data security and availability controls
- **PostgreSQL Best Practices** - Following PostgreSQL community standards

## 2. Master PRD Integration & Adaptive Schema Design

### Project Requirements Analysis Engine
```typescript
// Database architecture adapts based on Backend PRD and Master PRD requirements
interface DatabaseProjectContext {
  backendIntegration: BackendAPIRequirements;    // From Backend PRD
  securityRequirements: SecurityDataControls;   // From Security PRD
  frontendDataNeeds: FrontendDataContract;      // From Frontend PRD
  businessDomain: BusinessEntityModel;          // From Master PRD
  scalingRequirements: ScalingProjections;      // From Master PRD
  complianceLevel: ComplianceRequirements;      // From Master PRD
  performanceTargets: DatabasePerformanceKPIs;  // From Master PRD
}

// Schema Generation from Backend PRD Integration
interface BackendAPIRequirements {
  authentication: {
    userManagement: "users, roles, permissions, sessions";
    tokenStorage: "refresh_tokens, api_keys";
    auditLogging: "authentication_logs, security_events";
    mfaSupport: "user_mfa_settings, mfa_attempts";
  };
  
  coreEntities: {
    users: "id, email, password_hash, profile_data, preferences";
    projects: "id, name, description, owner_id, team_members, settings";
    tasks: "id, project_id, title, description, assignee_id, status, priority";
    comments: "id, task_id, user_id, content, created_at";
    attachments: "id, task_id, filename, file_path, mime_type, size";
  };
  
  relationships: {
    userProjects: "many-to-many with role assignment";
    projectTasks: "one-to-many with cascading rules";
    taskComments: "one-to-many with soft deletion";
    userAssignments: "many-to-many through task assignments";
  };
  
  apiSupport: {
    pagination: "efficient cursor-based pagination support";
    filtering: "indexed fields for common filter operations";
    sorting: "optimized indexes for sort operations";
    searching: "full-text search capabilities";
  };
}

// Security Integration from Security PRD
interface SecurityDataControls {
  encryption: {
    atRest: "AES-256 encryption for sensitive columns";
    inTransit: "TLS 1.3 for all database connections";
    keyManagement: "external key management service integration";
    columnLevel: "PII fields encrypted with different keys";
  };
  
  access: {
    rbac: "role-based access control at database level";
    rowLevel: "row-level security for multi-tenant data";
    auditLogging: "comprehensive audit trail for all operations";
    dataClassification: "classify and label sensitive data";
  };
  
  compliance: {
    gdpr: "data subject rights implementation";
    retention: "automated data lifecycle management";
    anonymization: "data anonymization for analytics";
    backup: "encrypted backups with compliance validation";
  };
}

// Database Architecture Generator
class DatabaseArchitectureGenerator {
  generateSchema(context: DatabaseProjectContext): DatabaseSchema {
    return {
      coreSchema: this.generateCoreSchema(context),
      securitySchema: this.generateSecuritySchema(context),
      analyticsSchema: this.generateAnalyticsSchema(context),
      indexes: this.generateOptimalIndexes(context),
      constraints: this.generateBusinessConstraints(context),
      triggers: this.generateAuditTriggers(context),
      functions: this.generateStoredProcedures(context),
      views: this.generateSecurityViews(context)
    };
  }
  
  private generateCoreSchema(context: DatabaseProjectContext): CoreSchema {
    // Adaptive schema based on business domain
    const baseEntities = ['users', 'organizations', 'projects', 'tasks'];
    const domainSpecificEntities = this.inferDomainEntities(context.businessDomain);
    
    return {
      entities: [...baseEntities, ...domainSpecificEntities],
      relationships: this.deriveRelationships(context),
      constraints: this.defineBusinessRules(context),
      indexes: this.optimizeForQueryPatterns(context.backendIntegration)
    };
  }
}
```

### Adaptive Database Selection Strategy
```typescript
// Database technology selection based on project characteristics
class DatabaseSelectionEngine {
  selectDatabaseStrategy(context: DatabaseProjectContext): DatabaseStrategy {
    const characteristics = this.analyzeDataCharacteristics(context);
    
    if (characteristics.transactional && characteristics.complex_relationships) {
      return {
        primary: 'postgresql-15',
        reasoning: 'ACID compliance with advanced features',
        configuration: this.getPostgreSQLConfig(context),
        extensions: ['uuid-ossp', 'pg_crypto', 'pg_stat_statements']
      };
    }
    
    if (characteristics.massive_scale && characteristics.simple_queries) {
      return {
        primary: 'postgresql-sharded',
        cache: 'redis-cluster',
        search: 'elasticsearch',
        reasoning: 'Horizontal scaling with specialized data stores'
      };
    }
    
    if (characteristics.real_time && characteristics.high_write_volume) {
      return {
        primary: 'postgresql',
        streaming: 'kafka',
        cache: 'redis',
        analytics: 'clickhouse',
        reasoning: 'Streaming architecture with real-time analytics'
      };
    }
    
    return {
      primary: 'postgresql',
      cache: 'redis',
      configuration: 'high-availability-setup'
    };
  }
}
```

## 3. Core Database Schema Design

### 3.1 Foundational Schema Architecture

**Primary Schema Design:**
```sql
-- Database: professional_dev_template
-- PostgreSQL 15+ with extensions

-- Enable required extensions
CREATE EXTENSION IF NOT EXISTS "uuid-ossp";
CREATE EXTENSION IF NOT EXISTS "pg_crypto";
CREATE EXTENSION IF NOT EXISTS "pg_stat_statements";
CREATE EXTENSION IF NOT EXISTS "pg_trgm";

-- Custom types for better type safety
CREATE TYPE user_status AS ENUM ('active', 'inactive', 'suspended', 'deleted');
CREATE TYPE project_status AS ENUM ('planning', 'active', 'on_hold', 'completed', 'archived');
CREATE TYPE task_status AS ENUM ('todo', 'in_progress', 'review', 'completed', 'cancelled');
CREATE TYPE task_priority AS ENUM ('low', 'medium', 'high', 'urgent');
CREATE TYPE notification_type AS ENUM ('info', 'warning', 'error', 'success');
CREATE TYPE audit_action AS ENUM ('INSERT', 'UPDATE', 'DELETE', 'SELECT');

-- Organizations (multi-tenancy support)
CREATE TABLE organizations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    name VARCHAR(255) NOT NULL,
    slug VARCHAR(100) UNIQUE NOT NULL,
    description TEXT,
    settings JSONB DEFAULT '{}',
    subscription_tier VARCHAR(50) DEFAULT 'free',
    subscription_expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Constraints
    CONSTRAINT valid_slug CHECK (slug ~ '^[a-z0-9-]+$'),
    CONSTRAINT valid_subscription_tier CHECK (subscription_tier IN ('free', 'pro', 'enterprise'))
);

-- Users with comprehensive profile management
CREATE TABLE users (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    email VARCHAR(255) UNIQUE NOT NULL,
    email_verified_at TIMESTAMPTZ,
    password_hash VARCHAR(255) NOT NULL,
    first_name VARCHAR(100) NOT NULL,
    last_name VARCHAR(100) NOT NULL,
    avatar_url VARCHAR(500),
    timezone VARCHAR(50) DEFAULT 'UTC',
    locale VARCHAR(10) DEFAULT 'en',
    status user_status DEFAULT 'active',
    last_login_at TIMESTAMPTZ,
    last_activity_at TIMESTAMPTZ,
    preferences JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Security constraints
    CONSTRAINT valid_email CHECK (email ~ '^[A-Za-z0-9._%+-]+@[A-Za-z0-9.-]+\.[A-Za-z]{2,}$'),
    CONSTRAINT valid_timezone CHECK (timezone IS NOT NULL),
    CONSTRAINT valid_locale CHECK (locale ~ '^[a-z]{2}(-[A-Z]{2})?$')
);

-- Roles and permissions system
CREATE TABLE roles (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    name VARCHAR(100) NOT NULL,
    description TEXT,
    permissions JSONB NOT NULL DEFAULT '[]',
    is_system BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(organization_id, name)
);

-- User role assignments
CREATE TABLE user_roles (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    role_id UUID REFERENCES roles(id) ON DELETE CASCADE,
    granted_by UUID REFERENCES users(id),
    granted_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ,
    
    UNIQUE(user_id, role_id)
);

-- Projects with comprehensive metadata
CREATE TABLE projects (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    name VARCHAR(255) NOT NULL,
    description TEXT,
    slug VARCHAR(100) NOT NULL,
    status project_status DEFAULT 'planning',
    owner_id UUID REFERENCES users(id) ON DELETE RESTRICT,
    start_date DATE,
    end_date DATE,
    budget DECIMAL(15,2),
    settings JSONB DEFAULT '{}',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_project_slug CHECK (slug ~ '^[a-z0-9-]+$'),
    CONSTRAINT valid_dates CHECK (end_date IS NULL OR start_date IS NULL OR end_date >= start_date),
    CONSTRAINT valid_budget CHECK (budget IS NULL OR budget >= 0),
    UNIQUE(organization_id, slug)
);

-- Project members with role-based access
CREATE TABLE project_members (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    role VARCHAR(50) NOT NULL DEFAULT 'member',
    permissions JSONB DEFAULT '[]',
    invited_by UUID REFERENCES users(id),
    joined_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_member_role CHECK (role IN ('owner', 'admin', 'member', 'viewer')),
    UNIQUE(project_id, user_id)
);

-- Tasks with comprehensive tracking
CREATE TABLE tasks (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    project_id UUID REFERENCES projects(id) ON DELETE CASCADE,
    parent_task_id UUID REFERENCES tasks(id) ON DELETE SET NULL,
    title VARCHAR(500) NOT NULL,
    description TEXT,
    status task_status DEFAULT 'todo',
    priority task_priority DEFAULT 'medium',
    assignee_id UUID REFERENCES users(id) ON DELETE SET NULL,
    reporter_id UUID REFERENCES users(id) ON DELETE SET NULL,
    estimated_hours DECIMAL(8,2),
    actual_hours DECIMAL(8,2),
    due_date TIMESTAMPTZ,
    completed_at TIMESTAMPTZ,
    tags TEXT[] DEFAULT '{}',
    labels JSONB DEFAULT '[]',
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_hours CHECK (
        (estimated_hours IS NULL OR estimated_hours >= 0) AND
        (actual_hours IS NULL OR actual_hours >= 0)
    ),
    CONSTRAINT valid_completion CHECK (
        (status = 'completed' AND completed_at IS NOT NULL) OR
        (status != 'completed' AND completed_at IS NULL)
    )
);

-- Task comments and activity
CREATE TABLE task_comments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    task_id UUID REFERENCES tasks(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id) ON DELETE SET NULL,
    content TEXT NOT NULL,
    content_type VARCHAR(20) DEFAULT 'text',
    parent_comment_id UUID REFERENCES task_comments(id) ON DELETE CASCADE,
    is_deleted BOOLEAN DEFAULT FALSE,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_content_type CHECK (content_type IN ('text', 'markdown', 'html')),
    CONSTRAINT non_empty_content CHECK (LENGTH(TRIM(content)) > 0)
);

-- File attachments
CREATE TABLE attachments (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    task_id UUID REFERENCES tasks(id) ON DELETE CASCADE,
    uploaded_by UUID REFERENCES users(id) ON DELETE SET NULL,
    filename VARCHAR(255) NOT NULL,
    original_filename VARCHAR(255) NOT NULL,
    file_path VARCHAR(1000) NOT NULL,
    file_size BIGINT NOT NULL,
    mime_type VARCHAR(100) NOT NULL,
    file_hash VARCHAR(64) NOT NULL,
    is_deleted BOOLEAN DEFAULT FALSE,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_file_size CHECK (file_size > 0),
    CONSTRAINT valid_mime_type CHECK (mime_type ~ '^[a-z-]+/[a-z0-9-+.]+$')
);

-- Activity tracking for audit and feeds
CREATE TABLE activities (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    user_id UUID REFERENCES users(id) ON DELETE SET NULL,
    action VARCHAR(100) NOT NULL,
    entity_type VARCHAR(50) NOT NULL,
    entity_id UUID NOT NULL,
    entity_name VARCHAR(255),
    description TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    ip_address INET,
    user_agent TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_entity_type CHECK (entity_type IN ('user', 'project', 'task', 'comment', 'attachment'))
);

-- Notifications system
CREATE TABLE notifications (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    type notification_type NOT NULL,
    title VARCHAR(255) NOT NULL,
    message TEXT NOT NULL,
    action_url VARCHAR(1000),
    is_read BOOLEAN DEFAULT FALSE,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMPTZ DEFAULT NOW(),
    read_at TIMESTAMPTZ,
    
    CONSTRAINT read_consistency CHECK (
        (is_read = TRUE AND read_at IS NOT NULL) OR
        (is_read = FALSE AND read_at IS NULL)
    )
);
```

### 3.2 Security and Audit Schema

**Security-Enhanced Schema Components:**
```sql
-- Security audit logging table
CREATE TABLE audit_logs (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(100) NOT NULL,
    operation audit_action NOT NULL,
    user_id UUID,
    organization_id UUID,
    record_id UUID,
    old_values JSONB,
    new_values JSONB,
    changed_columns TEXT[],
    ip_address INET,
    user_agent TEXT,
    session_id VARCHAR(255),
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    -- Index for performance
    INDEX idx_audit_logs_table_operation (table_name, operation),
    INDEX idx_audit_logs_user_time (user_id, created_at),
    INDEX idx_audit_logs_record (table_name, record_id)
);

-- Authentication and session management
CREATE TABLE user_sessions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    session_token VARCHAR(255) UNIQUE NOT NULL,
    refresh_token VARCHAR(255) UNIQUE,
    ip_address INET NOT NULL,
    user_agent TEXT,
    is_active BOOLEAN DEFAULT TRUE,
    last_activity_at TIMESTAMPTZ DEFAULT NOW(),
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_expiry CHECK (expires_at > created_at)
);

-- Multi-factor authentication settings
CREATE TABLE user_mfa_settings (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE UNIQUE,
    is_enabled BOOLEAN DEFAULT FALSE,
    secret_key VARCHAR(255),
    backup_codes TEXT[] DEFAULT '{}',
    last_used_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- API keys for service-to-service authentication
CREATE TABLE api_keys (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    organization_id UUID REFERENCES organizations(id) ON DELETE CASCADE,
    name VARCHAR(100) NOT NULL,
    key_hash VARCHAR(255) UNIQUE NOT NULL,
    key_prefix VARCHAR(20) NOT NULL,
    permissions JSONB DEFAULT '[]',
    is_active BOOLEAN DEFAULT TRUE,
    last_used_at TIMESTAMPTZ,
    expires_at TIMESTAMPTZ,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_key_expiry CHECK (expires_at IS NULL OR expires_at > created_at)
);

-- Password reset tokens
CREATE TABLE password_reset_tokens (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    token_hash VARCHAR(255) UNIQUE NOT NULL,
    is_used BOOLEAN DEFAULT FALSE,
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    used_at TIMESTAMPTZ,
    
    CONSTRAINT token_usage_consistency CHECK (
        (is_used = TRUE AND used_at IS NOT NULL) OR
        (is_used = FALSE AND used_at IS NULL)
    )
);

-- Email verification tokens
CREATE TABLE email_verification_tokens (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    user_id UUID REFERENCES users(id) ON DELETE CASCADE,
    email VARCHAR(255) NOT NULL,
    token_hash VARCHAR(255) UNIQUE NOT NULL,
    is_verified BOOLEAN DEFAULT FALSE,
    expires_at TIMESTAMPTZ NOT NULL,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    verified_at TIMESTAMPTZ,
    
    CONSTRAINT verification_consistency CHECK (
        (is_verified = TRUE AND verified_at IS NOT NULL) OR
        (is_verified = FALSE AND verified_at IS NULL)
    )
);
```

## 4. Performance Optimization & Indexing Strategy

### 4.1 Comprehensive Indexing Strategy

**Performance-Optimized Indexes:**
```sql
-- Primary table indexes for optimal query performance

-- Users table indexes
CREATE INDEX idx_users_organization_status ON users(organization_id, status) WHERE status = 'active';
CREATE INDEX idx_users_email_verified ON users(email) WHERE email_verified_at IS NOT NULL;
CREATE INDEX idx_users_last_activity ON users(last_activity_at DESC) WHERE status = 'active';
CREATE INDEX idx_users_search ON users USING gin(to_tsvector('english', first_name || ' ' || last_name || ' ' || email));

-- Projects table indexes
CREATE INDEX idx_projects_organization_status ON projects(organization_id, status);
CREATE INDEX idx_projects_owner ON projects(owner_id) WHERE status != 'archived';
CREATE INDEX idx_projects_dates ON projects(start_date, end_date) WHERE status = 'active';
CREATE INDEX idx_projects_search ON projects USING gin(to_tsvector('english', name || ' ' || COALESCE(description, '')));

-- Project members indexes
CREATE INDEX idx_project_members_project ON project_members(project_id, role);
CREATE INDEX idx_project_members_user ON project_members(user_id);
CREATE UNIQUE INDEX idx_project_members_unique ON project_members(project_id, user_id);

-- Tasks table indexes
CREATE INDEX idx_tasks_project_status ON tasks(project_id, status);
CREATE INDEX idx_tasks_assignee_status ON tasks(assignee_id, status) WHERE assignee_id IS NOT NULL;
CREATE INDEX idx_tasks_reporter ON tasks(reporter_id) WHERE reporter_id IS NOT NULL;
CREATE INDEX idx_tasks_priority_due ON tasks(priority, due_date) WHERE status != 'completed';
CREATE INDEX idx_tasks_parent ON tasks(parent_task_id) WHERE parent_task_id IS NOT NULL;
CREATE INDEX idx_tasks_search ON tasks USING gin(to_tsvector('english', title || ' ' || COALESCE(description, '')));
CREATE INDEX idx_tasks_tags ON tasks USING gin(tags);
CREATE INDEX idx_tasks_metadata ON tasks USING gin(metadata);

-- Task comments indexes
CREATE INDEX idx_task_comments_task ON task_comments(task_id, created_at DESC) WHERE is_deleted = FALSE;
CREATE INDEX idx_task_comments_user ON task_comments(user_id) WHERE is_deleted = FALSE;
CREATE INDEX idx_task_comments_parent ON task_comments(parent_comment_id) WHERE parent_comment_id IS NOT NULL;

-- Attachments indexes
CREATE INDEX idx_attachments_task ON attachments(task_id) WHERE is_deleted = FALSE;
CREATE INDEX idx_attachments_user ON attachments(uploaded_by) WHERE is_deleted = FALSE;
CREATE INDEX idx_attachments_type ON attachments(mime_type) WHERE is_deleted = FALSE;
CREATE INDEX idx_attachments_size ON attachments(file_size DESC) WHERE is_deleted = FALSE;

-- Activities indexes
CREATE INDEX idx_activities_organization_time ON activities(organization_id, created_at DESC);
CREATE INDEX idx_activities_user_time ON activities(user_id, created_at DESC) WHERE user_id IS NOT NULL;
CREATE INDEX idx_activities_entity ON activities(entity_type, entity_id);
CREATE INDEX idx_activities_action ON activities(action, created_at DESC);

-- Notifications indexes
CREATE INDEX idx_notifications_user_unread ON notifications(user_id, created_at DESC) WHERE is_read = FALSE;
CREATE INDEX idx_notifications_user_all ON notifications(user_id, created_at DESC);
CREATE INDEX idx_notifications_type ON notifications(type, created_at DESC);

-- Security and audit indexes
CREATE INDEX idx_audit_logs_table_time ON audit_logs(table_name, created_at DESC);
CREATE INDEX idx_audit_logs_user_time ON audit_logs(user_id, created_at DESC) WHERE user_id IS NOT NULL;
CREATE INDEX idx_audit_logs_record ON audit_logs(table_name, record_id, created_at DESC);

CREATE INDEX idx_user_sessions_token ON user_sessions(session_token) WHERE is_active = TRUE;
CREATE INDEX idx_user_sessions_user_active ON user_sessions(user_id, last_activity_at DESC) WHERE is_active = TRUE;
CREATE INDEX idx_user_sessions_expires ON user_sessions(expires_at) WHERE is_active = TRUE;

CREATE INDEX idx_api_keys_hash ON api_keys(key_hash) WHERE is_active = TRUE;
CREATE INDEX idx_api_keys_user_active ON api_keys(user_id, created_at DESC) WHERE is_active = TRUE;
CREATE INDEX idx_api_keys_expires ON api_keys(expires_at) WHERE is_active = TRUE AND expires_at IS NOT NULL;

-- Partial indexes for common queries
CREATE INDEX idx_active_projects ON projects(organization_id, updated_at DESC) WHERE status IN ('planning', 'active');
CREATE INDEX idx_pending_tasks ON tasks(assignee_id, due_date) WHERE status IN ('todo', 'in_progress') AND assignee_id IS NOT NULL;
CREATE INDEX idx_overdue_tasks ON tasks(project_id, due_date) WHERE status != 'completed' AND due_date < NOW();
```

## 5. Data Security & Compliance Implementation

### 5.1 Encryption and Data Protection

**Column-Level Encryption Implementation:**
```sql
-- Encryption functions for PII data
CREATE OR REPLACE FUNCTION encrypt_pii(data TEXT)
RETURNS TEXT AS $$
BEGIN
    -- Use pgcrypto for column-level encryption
    -- In production, use external key management service
    RETURN encode(pgp_sym_encrypt(data, current_setting('app.encryption_key')), 'base64');
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

CREATE OR REPLACE FUNCTION decrypt_pii(encrypted_data TEXT)
RETURNS TEXT AS $$
BEGIN
    IF encrypted_data IS NULL THEN
        RETURN NULL;
    END IF;
    RETURN pgp_sym_decrypt(decode(encrypted_data, 'base64'), current_setting('app.encryption_key'));
EXCEPTION
    WHEN OTHERS THEN
        -- Log decryption failure for security monitoring
        INSERT INTO audit_logs (table_name, operation, record_id, metadata)
        VALUES ('encryption', 'DECRYPT_FAILED', gen_random_uuid(), 
                jsonb_build_object('error', SQLERRM, 'timestamp', NOW()));
        RETURN '[DECRYPTION_FAILED]';
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Row-Level Security for multi-tenancy
ALTER TABLE users ENABLE ROW LEVEL SECURITY;
ALTER TABLE projects ENABLE ROW LEVEL SECURITY;
ALTER TABLE tasks ENABLE ROW LEVEL SECURITY;
ALTER TABLE activities ENABLE ROW LEVEL SECURITY;

-- RLS policies for organization isolation
CREATE POLICY users_org_isolation ON users
    FOR ALL
    TO application_role
    USING (organization_id = current_setting('app.current_organization_id')::UUID);

CREATE POLICY projects_org_isolation ON projects
    FOR ALL  
    TO application_role
    USING (organization_id = current_setting('app.current_organization_id')::UUID);

CREATE POLICY tasks_org_isolation ON tasks
    FOR ALL
    TO application_role
    USING (
        project_id IN (
            SELECT id FROM projects 
            WHERE organization_id = current_setting('app.current_organization_id')::UUID
        )
    );
```

### 5.2 GDPR Compliance Implementation

**Data Subject Rights Implementation:**
```sql
-- GDPR data mapping and classification
CREATE TABLE data_classification (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    table_name VARCHAR(100) NOT NULL,
    column_name VARCHAR(100) NOT NULL,
    data_category VARCHAR(50) NOT NULL, -- 'personal', 'sensitive', 'pseudonymized'
    retention_period INTERVAL,
    lawful_basis VARCHAR(100),
    processing_purpose TEXT,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    
    UNIQUE(table_name, column_name)
);

-- Insert data classification mappings
INSERT INTO data_classification (table_name, column_name, data_category, retention_period, lawful_basis, processing_purpose) VALUES
('users', 'email', 'personal', '7 years', 'contract', 'User authentication and communication'),
('users', 'first_name', 'personal', '7 years', 'contract', 'User identification and personalization'),
('users', 'last_name', 'personal', '7 years', 'contract', 'User identification and personalization'),
('users', 'avatar_url', 'personal', '7 years', 'consent', 'Profile personalization'),
('users', 'ip_address', 'personal', '1 year', 'legitimate_interest', 'Security and fraud prevention'),
('audit_logs', 'ip_address', 'personal', '3 years', 'legitimate_interest', 'Security monitoring'),
('user_sessions', 'ip_address', 'personal', '90 days', 'legitimate_interest', 'Security monitoring');

-- Data subject access request function
CREATE OR REPLACE FUNCTION generate_data_export(subject_user_id UUID)
RETURNS JSONB AS $$
DECLARE
    user_data JSONB;
    projects_data JSONB;
    tasks_data JSONB;
    comments_data JSONB;
    activities_data JSONB;
    export_data JSONB;
BEGIN
    -- User profile data
    SELECT to_jsonb(u) INTO user_data
    FROM (
        SELECT id, email, first_name, last_name, timezone, locale, 
               preferences, created_at, updated_at
        FROM users WHERE id = subject_user_id
    ) u;
    
    -- Projects data (as member)
    SELECT jsonb_agg(to_jsonb(p)) INTO projects_data
    FROM (
        SELECT pr.id, pr.name, pr.description, pm.role, pm.joined_at
        FROM projects pr
        JOIN project_members pm ON pr.id = pm.project_id
        WHERE pm.user_id = subject_user_id
    ) p;
    
    -- Tasks data (assigned or created)
    SELECT jsonb_agg(to_jsonb(t)) INTO tasks_data
    FROM (
        SELECT id, title, description, status, priority, 
               estimated_hours, actual_hours, created_at, updated_at
        FROM tasks 
        WHERE assignee_id = subject_user_id OR reporter_id = subject_user_id
    ) t;
    
    -- Comments data
    SELECT jsonb_agg(to_jsonb(c)) INTO comments_data
    FROM (
        SELECT id, task_id, content, created_at, updated_at
        FROM task_comments 
        WHERE user_id = subject_user_id AND is_deleted = FALSE
    ) c;
    
    -- Activity data
    SELECT jsonb_agg(to_jsonb(a)) INTO activities_data
    FROM (
        SELECT action, entity_type, entity_name, description, created_at
        FROM activities 
        WHERE user_id = subject_user_id
        ORDER BY created_at DESC
        LIMIT 1000  -- Limit for performance
    ) a;
    
    -- Combine all data
    export_data := jsonb_build_object(
        'user_profile', user_data,
        'projects', COALESCE(projects_data, '[]'::jsonb),
        'tasks', COALESCE(tasks_data, '[]'::jsonb),
        'comments', COALESCE(comments_data, '[]'::jsonb),
        'activities', COALESCE(activities_data, '[]'::jsonb),
        'export_generated_at', NOW(),
        'retention_notice', 'Data retention periods vary by data type. Contact support for specific retention information.'
    );
    
    -- Log the data export request
    INSERT INTO audit_logs (table_name, operation, user_id, record_id, metadata)
    VALUES ('gdpr_exports', 'SELECT', subject_user_id, subject_user_id,
            jsonb_build_object('export_size_bytes', length(export_data::text)));
    
    RETURN export_data;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;
```

## 6. Advanced Database Features & Triggers

### 6.1 Audit Triggers and Change Tracking

**Comprehensive Audit System:**
```sql
-- Generic audit trigger function
CREATE OR REPLACE FUNCTION audit_trigger_function()
RETURNS TRIGGER AS $$
DECLARE
    old_values JSONB;
    new_values JSONB;
    changed_columns TEXT[];
    current_user_id UUID;
    current_org_id UUID;
BEGIN
    -- Get current session context
    current_user_id := NULLIF(current_setting('app.current_user_id', true), '')::UUID;
    current_org_id := NULLIF(current_setting('app.current_organization_id', true), '')::UUID;
    
    -- Handle different operations
    IF TG_OP = 'DELETE' THEN
        old_values := to_jsonb(OLD);
        new_values := NULL;
        
        INSERT INTO audit_logs (
            table_name, operation, user_id, organization_id, record_id,
            old_values, new_values, changed_columns,
            ip_address, user_agent, session_id
        ) VALUES (
            TG_TABLE_NAME, TG_OP::audit_action, current_user_id, current_org_id,
            (OLD.id),
            old_values, new_values, NULL,
            NULLIF(current_setting('app.client_ip', true), '')::INET,
            NULLIF(current_setting('app.user_agent', true), ''),
            NULLIF(current_setting('app.session_id', true), '')
        );
        
        RETURN OLD;
        
    ELSIF TG_OP = 'UPDATE' THEN
        old_values := to_jsonb(OLD);
        new_values := to_jsonb(NEW);
        
        -- Identify changed columns
        SELECT array_agg(key) INTO changed_columns
        FROM jsonb_each_text(old_values) o
        JOIN jsonb_each_text(new_values) n ON o.key = n.key
        WHERE o.value IS DISTINCT FROM n.value;
        
        -- Only log if there were actual changes
        IF array_length(changed_columns, 1) > 0 THEN
            INSERT INTO audit_logs (
                table_name, operation, user_id, organization_id, record_id,
                old_values, new_values, changed_columns,
                ip_address, user_agent, session_id
            ) VALUES (
                TG_TABLE_NAME, TG_OP::audit_action, current_user_id, current_org_id,
                (NEW.id),
                old_values, new_values, changed_columns,
                NULLIF(current_setting('app.client_ip', true), '')::INET,
                NULLIF(current_setting('app.user_agent', true), ''),
                NULLIF(current_setting('app.session_id', true), '')
            );
        END IF;
        
        RETURN NEW;
        
    ELSIF TG_OP = 'INSERT' THEN
        new_values := to_jsonb(NEW);
        
        INSERT INTO audit_logs (
            table_name, operation, user_id, organization_id, record_id,
            old_values, new_values, changed_columns,
            ip_address, user_agent, session_id
        ) VALUES (
            TG_TABLE_NAME, TG_OP::audit_action, current_user_id, current_org_id,
            (NEW.id),
            NULL, new_values, NULL,
            NULLIF(current_setting('app.client_ip', true), '')::INET,
            NULLIF(current_setting('app.user_agent', true), ''),
            NULLIF(current_setting('app.session_id', true), '')
        );
        
        RETURN NEW;
    END IF;
    
    RETURN NULL;
END;
$$ LANGUAGE plpgsql SECURITY DEFINER;

-- Create audit triggers for key tables
CREATE TRIGGER audit_users_trigger
    AFTER INSERT OR UPDATE OR DELETE ON users
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

CREATE TRIGGER audit_projects_trigger
    AFTER INSERT OR UPDATE OR DELETE ON projects
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

CREATE TRIGGER audit_tasks_trigger
    AFTER INSERT OR UPDATE OR DELETE ON tasks
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();

CREATE TRIGGER audit_task_comments_trigger
    AFTER INSERT OR UPDATE OR DELETE ON task_comments
    FOR EACH ROW EXECUTE FUNCTION audit_trigger_function();
```

## 7. Backup and Disaster Recovery

### 7.1 Comprehensive Backup Strategy

**Automated Backup System:**
```sql
-- Backup configuration table
CREATE TABLE backup_configurations (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    backup_type VARCHAR(50) NOT NULL, -- 'full', 'incremental', 'differential'
    schedule_expression VARCHAR(100) NOT NULL, -- Cron expression
    retention_days INTEGER NOT NULL,
    compression_enabled BOOLEAN DEFAULT TRUE,
    encryption_enabled BOOLEAN DEFAULT TRUE,
    storage_location VARCHAR(500) NOT NULL,
    is_active BOOLEAN DEFAULT TRUE,
    created_at TIMESTAMPTZ DEFAULT NOW(),
    updated_at TIMESTAMPTZ DEFAULT NOW()
);

-- Backup execution log
CREATE TABLE backup_executions (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    configuration_id UUID REFERENCES backup_configurations(id),
    backup_type VARCHAR(50) NOT NULL,
    status VARCHAR(50) NOT NULL, -- 'running', 'completed', 'failed', 'cancelled'
    start_time TIMESTAMPTZ NOT NULL,
    end_time TIMESTAMPTZ,
    file_path VARCHAR(1000),
    file_size_bytes BIGINT,
    checksum VARCHAR(128),
    error_message TEXT,
    metadata JSONB DEFAULT '{}',
    
    CONSTRAINT valid_status CHECK (status IN ('running', 'completed', 'failed', 'cancelled'))
);

-- Initialize backup configurations
INSERT INTO backup_configurations (backup_type, schedule_expression, retention_days, storage_location) VALUES
('full', '0 2 * * 0', 30, 's3://backup-bucket/database/full/'),     -- Weekly full backup
('incremental', '0 2 * * 1-6', 7, 's3://backup-bucket/database/incremental/'), -- Daily incremental
('differential', '0 6,18 * * *', 3, 's3://backup-bucket/database/differential/'); -- Twice daily differential
```

## 8. Analytics and Reporting Schema

### 8.1 Data Warehouse Design

**Analytics Schema for Business Intelligence:**
```sql
-- Create analytics schema
CREATE SCHEMA IF NOT EXISTS analytics;

-- Time dimension table
CREATE TABLE analytics.dim_time (
    time_key INTEGER PRIMARY KEY,
    full_date DATE UNIQUE NOT NULL,
    year INTEGER NOT NULL,
    quarter INTEGER NOT NULL,
    month INTEGER NOT NULL,
    week INTEGER NOT NULL,
    day_of_month INTEGER NOT NULL,
    day_of_week INTEGER NOT NULL,
    day_of_year INTEGER NOT NULL,
    month_name VARCHAR(20) NOT NULL,
    day_name VARCHAR(20) NOT NULL,
    is_weekend BOOLEAN NOT NULL,
    is_holiday BOOLEAN DEFAULT FALSE,
    fiscal_year INTEGER,
    fiscal_quarter INTEGER
);

-- Daily user activity fact table
CREATE TABLE analytics.fact_user_activity (
    activity_date DATE NOT NULL,
    user_key UUID NOT NULL,
    organization_id UUID NOT NULL,
    login_count INTEGER DEFAULT 0,
    session_duration_minutes INTEGER DEFAULT 0,
    tasks_created INTEGER DEFAULT 0,
    tasks_updated INTEGER DEFAULT 0,
    tasks_completed INTEGER DEFAULT 0,
    comments_posted INTEGER DEFAULT 0,
    projects_accessed INTEGER DEFAULT 0,
    last_activity_time TIMESTAMPTZ,
    
    PRIMARY KEY (activity_date, user_key)
);

-- Project performance fact table
CREATE TABLE analytics.fact_project_performance (
    date_key INTEGER NOT NULL,
    project_key UUID NOT NULL,
    organization_id UUID NOT NULL,
    
    -- Task metrics
    total_tasks INTEGER DEFAULT 0,
    completed_tasks INTEGER DEFAULT 0,
    overdue_tasks INTEGER DEFAULT 0,
    tasks_created_today INTEGER DEFAULT 0,
    tasks_completed_today INTEGER DEFAULT 0,
    
    -- Time metrics
    total_estimated_hours DECIMAL(10,2) DEFAULT 0,
    total_actual_hours DECIMAL(10,2) DEFAULT 0,
    avg_completion_time_days DECIMAL(8,2),
    
    -- Team metrics
    active_team_members INTEGER DEFAULT 0,
    team_velocity DECIMAL(8,2), -- tasks per day
    
    PRIMARY KEY (date_key, project_key)
);
```

## 9. Database Monitoring and Health Checks

### 9.1 Performance Monitoring Framework

**Comprehensive Database Monitoring:**
```sql
-- Database health monitoring table
CREATE TABLE db_health_metrics (
    id UUID PRIMARY KEY DEFAULT uuid_generate_v4(),
    metric_name VARCHAR(100) NOT NULL,
    metric_value DECIMAL(15,4) NOT NULL,
    metric_unit VARCHAR(20),
    threshold_warning DECIMAL(15,4),
    threshold_critical DECIMAL(15,4),
    status VARCHAR(20) DEFAULT 'ok', -- 'ok', 'warning', 'critical'
    metadata JSONB DEFAULT '{}',
    recorded_at TIMESTAMPTZ DEFAULT NOW(),
    
    CONSTRAINT valid_status CHECK (status IN ('ok', 'warning', 'critical'))
);

-- Function to collect database metrics
CREATE OR REPLACE FUNCTION collect_database_metrics()
RETURNS INTEGER AS $$
DECLARE
    metric_count INTEGER := 0;
    current_time TIMESTAMPTZ := NOW();
BEGIN
    -- Connection metrics
    INSERT INTO db_health_metrics (metric_name, metric_value, metric_unit, threshold_warning, threshold_critical, status)
    SELECT 
        'active_connections',
        count(*),
        'connections',
        80,
        95,
        CASE 
            WHEN count(*) >= 95 THEN 'critical'
            WHEN count(*) >= 80 THEN 'warning'
            ELSE 'ok'
        END
    FROM pg_stat_activity
    WHERE state = 'active';
    
    -- Database size metrics
    INSERT INTO db_health_metrics (metric_name, metric_value, metric_unit, threshold_warning, threshold_critical, status)
    SELECT 
        'database_size_gb',
        pg_database_size(current_database()) / 1024.0^3,
        'GB',
        100,
        200,
        CASE 
            WHEN pg_database_size(current_database()) / 1024.0^3 >= 200 THEN 'critical'
            WHEN pg_database_size(current_database()) / 1024.0^3 >= 100 THEN 'warning'
            ELSE 'ok'
        END;
    
    -- Cache hit ratio
    INSERT INTO db_health_metrics (metric_name, metric_value, metric_unit, threshold_warning, threshold_critical, status)
    SELECT 
        'cache_hit_ratio',
        CASE WHEN blks_hit + blks_read > 0 
             THEN (blks_hit::FLOAT / (blks_hit + blks_read)) * 100
             ELSE 100 END,
        'percent',
        90,
        80,
        CASE 
            WHEN CASE WHEN blks_hit + blks_read > 0 
                      THEN (blks_hit::FLOAT / (blks_hit + blks_read)) * 100
                      ELSE 100 END < 80 THEN 'critical'
            WHEN CASE WHEN blks_hit + blks_read > 0 
                      THEN (blks_hit::FLOAT / (blks_hit + blks_read)) * 100
                      ELSE 100 END < 90 THEN 'warning'
            ELSE 'ok'
        END
    FROM pg_stat_database 
    WHERE datname = current_database();
    
    GET DIAGNOSTICS metric_count = ROW_COUNT;
    
    -- Clean up old metrics (keep 7 days)
    DELETE FROM db_health_metrics 
    WHERE recorded_at < NOW() - interval '7 days';
    
    RETURN metric_count;
END;
$$ LANGUAGE plpgsql;
```

## 10. Integration Requirements for Other PRDs

### 10.1 Backend Integration Points

**Database Service Interfaces for Backend PRD:**
```typescript
// Database service interfaces that Backend PRD must implement
interface DatabaseIntegrationContract {
  connectionManagement: {
    poolConfiguration: {
      maxConnections: 20;
      minConnections: 5;
      acquireTimeoutMillis: 60000;
      idleTimeoutMillis: 30000;
      connectionTimeoutMillis: 30000;
    };
    
    connectionSecurity: {
      sslMode: "require";
      certificateValidation: true;
      encryptedConnections: true;
      connectionAuditing: true;
    };
    
    readWriteSplitting: {
      writeOperations: "primary-database-only";
      readOperations: "read-replicas-with-fallback";
      connectionRouting: "automatic-based-on-operation-type";
    };
  };
  
  queryInterface: {
    orm: "prisma-with-type-generation";
    rawQueries: "parameterized-queries-only";
    transactionSupport: "nested-transactions-with-savepoints";
    migrationSupport: "automated-schema-migrations";
  };
  
  cacheIntegration: {
    queryResultCaching: "redis-with-intelligent-invalidation";
    sessionCaching: "redis-cluster-for-sessions";
    cacheWarmup: "automated-cache-warming-on-deployment";
    cacheInvalidation: "tag-based-invalidation-strategy";
  };
  
  realTimeSupport: {
    changeStreaming: "postgresql-logical-replication";
    webhookTriggers: "database-trigger-based-notifications";
    eventSourcing: "audit-log-based-event-reconstruction";
  };
}
```

### 10.2 Frontend Integration Requirements

**Database Query Patterns for Frontend PRD:**
```sql
-- Optimized queries for Frontend data requirements
CREATE VIEW frontend_dashboard_data AS
SELECT 
    u.id as user_id,
    u.organization_id,
    
    -- User stats
    jsonb_build_object(
        'active_projects', (
            SELECT COUNT(*) FROM project_members pm 
            JOIN projects p ON pm.project_id = p.id 
            WHERE pm.user_id = u.id AND p.status IN ('planning', 'active')
        ),
        'assigned_tasks', (
            SELECT COUNT(*) FROM tasks t 
            JOIN projects p ON t.project_id = p.id
            WHERE t.assignee_id = u.id AND t.status IN ('todo', 'in_progress') AND p.status != 'archived'
        ),
        'completed_this_week', (
            SELECT COUNT(*) FROM tasks t
            WHERE t.assignee_id = u.id 
            AND t.completed_at >= date_trunc('week', NOW())
            AND t.status = 'completed'
        ),
        'overdue_tasks', (
            SELECT COUNT(*) FROM tasks t
            JOIN projects p ON t.project_id = p.id
            WHERE t.assignee_id = u.id 
            AND t.due_date < NOW()
            AND t.status NOT IN ('completed', 'cancelled')
            AND p.status != 'archived'
        )
    ) as user_stats
    
FROM users u
WHERE u.status = 'active';

-- Efficient project listing for Frontend
CREATE OR REPLACE FUNCTION get_user_projects(
    p_user_id UUID,
    p_limit INTEGER DEFAULT 20,
    p_offset INTEGER DEFAULT 0,
    p_status project_status[] DEFAULT NULL,
    p_search TEXT DEFAULT NULL
)
RETURNS TABLE(
    id UUID,
    name VARCHAR(255),
    description TEXT,
    status project_status,
    owner_name TEXT,
    member_count BIGINT,
    task_count BIGINT,
    completed_task_count BIGINT,
    last_activity TIMESTAMPTZ,
    user_role VARCHAR(50),
    created_at TIMESTAMPTZ,
    updated_at TIMESTAMPTZ
) AS $$
BEGIN
    RETURN QUERY
    SELECT 
        p.id,
        p.name,
        p.description,
        p.status,
        CONCAT(owner.first_name, ' ', owner.last_name) as owner_name,
        (SELECT COUNT(*) FROM project_members pm WHERE pm.project_id = p.id) as member_count,
        (SELECT COUNT(*) FROM tasks t WHERE t.project_id = p.id) as task_count,
        (SELECT COUNT(*) FROM tasks t WHERE t.project_id = p.id AND t.status = 'completed') as completed_task_count,
        GREATEST(p.updated_at, (SELECT MAX(t.updated_at) FROM tasks t WHERE t.project_id = p.id)) as last_activity,
        pm.role as user_role,
        p.created_at,
        p.updated_at
    FROM projects p
    JOIN project_members pm ON p.id = pm.project_id
    JOIN users owner ON p.owner_id = owner.id
    WHERE pm.user_id = p_user_id
    AND (p_status IS NULL OR p.status = ANY(p_status))
    AND (p_search IS NULL OR p.name ILIKE '%' || p_search || '%' OR p.description ILIKE '%' || p_search || '%')
    ORDER BY last_activity DESC
    LIMIT p_limit
    OFFSET p_offset;
END;
$$ LANGUAGE plpgsql;
```

## 11. Validation Criteria & Quality Gates

### 11.1 Database Implementation Acceptance Criteria

**Comprehensive Database Validation Checklist:**
```markdown
## Schema Design & Structure
- [ ] All tables have primary keys (UUID type for business entities)
- [ ] Foreign key relationships properly defined with appropriate cascade rules
- [ ] Check constraints implemented for data validation
- [ ] Proper use of NOT NULL constraints for required fields
- [ ] Appropriate data types selected for all columns
- [ ] Enum types used for controlled vocabularies
- [ ] JSONB columns used appropriately for flexible schema requirements
- [ ] Database schema documented with comments on tables and columns

## Performance & Indexing
- [ ] Primary indexes on all frequently queried columns
- [ ] Composite indexes for multi-column query patterns
- [ ] Partial indexes for filtered queries
- [ ] Full-text search indexes for search functionality
- [ ] Query performance meets targets (p95 < 100ms for simple queries)
- [ ] No missing indexes identified by query analysis
- [ ] Index maintenance strategy documented and automated
- [ ] Database statistics updated and query plans optimized

## Security & Compliance
- [ ] Row-level security (RLS) policies implemented for multi-tenancy
- [ ] Column-level encryption for sensitive data (PII)
- [ ] Database roles configured with principle of least privilege
- [ ] Audit triggers capturing all data modifications
- [ ] Data classification mapping completed and documented
- [ ] GDPR compliance functions implemented and tested
- [ ] Backup encryption enabled and tested
- [ ] Connection encryption enforced (TLS 1.3)

## Backup & Recovery
- [ ] Automated backup strategy implemented (full, incremental, differential)
- [ ] Point-in-time recovery (PITR) configured and tested
- [ ] Recovery procedures documented and tested
- [ ] Backup integrity verification automated
- [ ] Cross-region backup replication configured
- [ ] Disaster recovery runbook created and validated
- [ ] Recovery time objective (RTO) < 4 hours achieved
- [ ] Recovery point objective (RPO) < 15 minutes achieved
```

### 11.2 Performance Benchmarks

**Database Performance Targets:**
```sql
-- Performance monitoring and validation queries
CREATE OR REPLACE FUNCTION validate_performance_benchmarks()
RETURNS TABLE(
    benchmark_name TEXT,
    target_value TEXT,
    current_value TEXT,
    status TEXT,
    details JSONB
) AS $$
BEGIN
    RETURN QUERY
    -- Query response time benchmark
    SELECT 
        'average_query_response_time'::TEXT,
        '< 100ms for simple queries'::TEXT,
        COALESCE(
            (SELECT avg(total_time)::TEXT || 'ms' 
             FROM pg_stat_statements 
             WHERE calls > 100), 
            'No data'
        )::TEXT,
        CASE 
            WHEN (SELECT avg(total_time) FROM pg_stat_statements WHERE calls > 100) < 100 THEN 'PASS'
            ELSE 'FAIL'
        END::TEXT,
        jsonb_build_object(
            'measurement_period', '24_hours',
            'sample_size', (SELECT COUNT(*) FROM pg_stat_statements WHERE calls > 100)
        )
    
    UNION ALL
    
    -- Connection utilization benchmark
    SELECT 
        'connection_utilization'::TEXT,
        '< 80% of max connections'::TEXT,
        (
            SELECT ROUND((COUNT(*)::FLOAT / current_setting('max_connections')::INT * 100), 2)::TEXT || '%'
            FROM pg_stat_activity
        )::TEXT,
        CASE 
            WHEN (
                SELECT COUNT(*)::FLOAT / current_setting('max_connections')::INT
                FROM pg_stat_activity
            ) < 0.8 THEN 'PASS'
            ELSE 'FAIL'
        END::TEXT,
        jsonb_build_object(
            'active_connections', (SELECT COUNT(*) FROM pg_stat_activity),
            'max_connections', current_setting('max_connections')::INT
        );
END;
$$ LANGUAGE plpgsql;
```

## 12. Implementation Roadmap & Handoff

### 12.1 Database Development Phases

**Phase 1: Foundation & Core Schema (Weeks 1-2)**
```markdown
## Core Schema Implementation
- [ ] Set up PostgreSQL 15 with required extensions
- [ ] Implement core entity tables (users, organizations, projects, tasks)
- [ ] Create security tables (sessions, audit logs, roles)
- [ ] Set up basic relationships and constraints
- [ ] Implement column-level encryption for PII
- [ ] Create audit triggers for change tracking
- [ ] Set up row-level security policies
- [ ] Configure database roles and permissions

## Initial Performance Setup
- [ ] Create primary indexes on all core tables
- [ ] Implement basic query optimization
- [ ] Set up connection pooling configuration
- [ ] Configure basic monitoring and health checks
- [ ] Establish backup procedures (daily full backup)
- [ ] Create development and staging databases
- [ ] Document initial schema and setup procedures

## Deliverables
- Complete core database schema
- Basic security implementation
- Initial performance optimization
- Backup and recovery procedures
- Database documentation
```

**Phase 2: Advanced Features & Performance (Weeks 3-4)**
```markdown
## Advanced Schema Features
- [ ] Implement analytics schema and ETL processes
- [ ] Create materialized views for complex queries
- [ ] Set up full-text search capabilities
- [ ] Implement business rule validation triggers
- [ ] Add comprehensive indexing strategy
- [ ] Create stored procedures for complex operations
- [ ] Implement data retention and cleanup automation
- [ ] Set up notification and activity tracking

## Performance Optimization
- [ ] Optimize query performance with advanced indexing
- [ ] Implement query result caching strategies
- [ ] Set up read replica configuration
- [ ] Create performance monitoring dashboards
- [ ] Implement automated maintenance tasks
- [ ] Optimize connection pooling and scaling
- [ ] Conduct performance testing and tuning
- [ ] Document performance benchmarks

## Deliverables
- Advanced analytics capabilities
- Optimized performance meeting benchmarks
- Comprehensive monitoring and alerting
- Automated maintenance procedures
- Performance documentation and runbooks
```

**Phase 3: Production Readiness & Integration (Weeks 5-6)**
```markdown
## Production Preparation
- [ ] Implement high availability with failover
- [ ] Set up cross-region backup replication
- [ ] Complete security hardening and penetration testing
- [ ] Implement comprehensive monitoring and alerting
- [ ] Create disaster recovery procedures and test them
- [ ] Optimize for production workloads
- [ ] Complete compliance validation (GDPR, SOC2)
- [ ] Create operational runbooks and procedures

## Integration Testing
- [ ] Test integration with Backend PRD API layer
- [ ] Validate Frontend PRD query patterns and performance
- [ ] Test Security PRD compliance features
- [ ] Validate real-time streaming and WebSocket integration
- [ ] Test backup and recovery procedures
- [ ] Conduct load testing with realistic data volumes
- [ ] Validate migration procedures
- [ ] Complete end-to-end system testing

## Deliverables
- Production-ready database system
- Complete integration with all other PRDs
- Validated disaster recovery procedures
- Comprehensive operational documentation
- Performance benchmarks and monitoring
```

### 12.2 Database Team Handoff Package

**Technical Documentation:**
```markdown
## Database Architecture Documentation
1. **Schema Design Document** - Complete ERD and table specifications
2. **Security Implementation Guide** - Encryption, access control, compliance
3. **Performance Tuning Guide** - Indexing strategies and optimization techniques
4. **Backup and Recovery Procedures** - Complete disaster recovery playbook
5. **Monitoring and Alerting Setup** - Health checks and performance monitoring
6. **Migration and Schema Evolution** - Change management procedures
7. **Integration Specifications** - API contracts and data access patterns
8. **Compliance Documentation** - GDPR, audit, and regulatory requirements

## Operational Runbooks
1. **Daily Operations Checklist** - Routine maintenance and monitoring tasks
2. **Incident Response Procedures** - Database emergency response protocols
3. **Performance Troubleshooting** - Query optimization and bottleneck resolution
4. **Scaling Procedures** - Horizontal and vertical scaling protocols
5. **Security Incident Response** - Data breach and security event procedures
6. **Backup Validation and Recovery** - Regular backup testing and restoration
7. **Schema Change Management** - Safe deployment of database changes
8. **Compliance Audit Procedures** - GDPR and regulatory compliance workflows

## Development Tools and Scripts
1. **Database Setup Scripts** - Complete environment provisioning automation
2. **Migration Scripts** - Schema evolution and data migration tools
3. **Monitoring Scripts** - Custom health checks and alerting tools
4. **Performance Analysis Tools** - Query analysis and optimization utilities
5. **Backup and Recovery Scripts** - Automated backup and restoration tools
6. **Data Generation Scripts** - Test data creation and anonymization
7. **Compliance Tools** - GDPR data export and anonymization utilities
8. **Integration Testing Scripts** - API and system integration validation
```

**Training Materials:**
```markdown
## Database Administration Training
1. **PostgreSQL Advanced Administration** - Configuration, tuning, and optimization
2. **Security Best Practices** - Encryption, access control, and compliance
3. **Performance Monitoring** - Metrics analysis and troubleshooting techniques
4. **Backup and Recovery** - Disaster recovery planning and execution
5. **High Availability Setup** - Replication, failover, and clustering
6. **Query Optimization** - Index design and query performance tuning
7. **Schema Design Patterns** - Best practices for relational design
8. **Compliance Management** - GDPR, audit logging, and data governance

## Development Team Training
1. **Database Integration Patterns** - ORM usage and query optimization
2. **Security Requirements** - Data protection and access control
3. **Performance Best Practices** - Efficient query patterns and caching
4. **Migration Procedures** - Safe schema changes and data migrations
5. **Monitoring and Alerting** - Database health and performance monitoring
6. **Troubleshooting Common Issues** - Connection pooling, locks, and deadlocks
7. **Backup and Recovery Awareness** - Understanding RTO/RPO requirements
8. **Compliance Responsibilities** - GDPR and data handling requirements
```

## 13. Summary & Cross-PRD Integration

### 13.1 Database Architecture PRD Completion Summary

This Database Architecture PRD provides the comprehensive data foundation that integrates seamlessly with all other PRDs in the professional development template. The database architecture is designed to:

 **Scale from startup to enterprise** with proper indexing, partitioning, and replication strategies  
 **Integrate seamlessly** with Backend, Frontend, and Security PRD requirements  
 **Ensure data integrity** with comprehensive validation, constraints, and business rules  
 **Provide exceptional performance** with sub-100ms query response times and 99.9% availability  
 **Maintain security compliance** with encryption, audit logging, and GDPR requirements  
 **Enable rapid development** with optimized query patterns and analytical capabilities  

### 13.2 Key Integration Points Achieved

**Security PRD Integration:**
- **Data Encryption** implements AES-256-GCM for sensitive columns following Security PRD specifications
- **Access Control** provides RBAC and row-level security as defined in Security PRD requirements
- **Audit Logging** captures all security-relevant database operations per Security PRD standards
- **GDPR Compliance** implements data subject rights and retention policies from Security PRD
- **Database Security** follows all Security PRD database protection requirements

**Backend PRD Dependencies Fulfilled:**
- **API Support** provides optimized queries for all Backend PRD REST endpoints
- **Real-time Features** supports WebSocket data streaming through logical replication
- **Authentication Data** stores JWT tokens and session management per Backend PRD specifications
- **Performance Requirements** meets Backend PRD response time and throughput targets
- **Caching Integration** provides Redis integration points for Backend PRD caching strategy

**Frontend PRD Data Requirements:**
- **Dashboard Queries** optimized views and functions for Frontend PRD dashboard components
- **Real-time Updates** database triggers and change streaming for Frontend WebSocket features
- **Search Capabilities** full-text search and filtering support for Frontend PRD search features
- **Pagination Support** efficient cursor-based pagination for Frontend PRD list components
- **Analytics Data** aggregated views and metrics for Frontend PRD charts and reporting

### 13.3 Professional Standards Summary

This Database Architecture PRD demonstrates how Maria Santos's 14+ years of experience creates a production-ready database foundation:

**Netflix/Uber/Amazon-Level Patterns:**
- **Scalable Architecture** supporting millions of users with proper sharding and replication
- **Performance Engineering** achieving sub-100ms response times with advanced indexing
- **Data Security** implementing comprehensive encryption and access control
- **Analytics Architecture** providing real-time insights and business intelligence
- **Operational Excellence** with automated monitoring, maintenance, and disaster recovery

**Professional Decision Framework:**
- **Consistency First** - ACID compliance with careful consideration of performance trade-offs
- **Security by Design** - Encryption, access control, and audit logging built into every layer
- **Performance Always** - Optimized queries, proper indexing, and caching strategies
- **Automation Everything** - Automated backups, monitoring, scaling, and maintenance
- **Data Quality** - Comprehensive validation, constraints, and business rule enforcement

### 13.4 Database PRD Conclusion

This Database Architecture PRD completes the core technical foundation of the professional development template by providing:

**Complete Data Foundation:**
- **Comprehensive Schema Design** supporting all application requirements
- **Advanced Security Implementation** with encryption, access control, and compliance
- **High-Performance Architecture** with optimization for scale and speed
- **Operational Excellence** with monitoring, backup, and disaster recovery
- **Seamless Integration** with Backend, Frontend, and Security PRDs

**Production-Ready Features:**
- **Scalability** from startup to enterprise with proven scaling patterns
- **Security** meeting enterprise compliance requirements (GDPR, SOC2)
- **Performance** achieving sub-100ms query response times
- **Reliability** with 99.9% availability and comprehensive backup strategies
- **Maintainability** with automated operations and clear documentation

**Development Velocity:**
- **Developer-Friendly** with type-safe ORM integration and clear patterns
- **Well-Documented** with comprehensive guides and operational runbooks
- **Battle-Tested** patterns from Netflix, Uber, and Amazon-scale deployments
- **Future-Proof** architecture ready for advanced features and scaling

The Database Architecture PRD establishes the robust data foundation needed for any professional application, ensuring data integrity, performance, security, and scalability while providing seamless integration with all other system components.

**Ready to generate the Infrastructure/DevOps PRD next to complete the deployment foundation?**